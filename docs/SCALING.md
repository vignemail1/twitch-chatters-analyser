# Guide de ScalabilitÃ© et Performance

Ce document dÃ©crit les optimisations de performance et la stratÃ©gie de scalabilitÃ© du projet.

## Table des matiÃ¨res

- [Optimisations Verticales](#optimisations-verticales)
- [ScalabilitÃ© Horizontale](#scalabilitÃ©-horizontale)
- [Redis - Cache DistribuÃ©](#redis---cache-distribuÃ©)
- [Configuration](#configuration)
- [Monitoring](#monitoring)

## Optimisations Verticales

### Indexes Base de DonnÃ©es

Des indexes composÃ©s ont Ã©tÃ© ajoutÃ©s pour optimiser les requÃªtes frÃ©quentes :

```sql
-- Sessions : lookup rapide par user + status
INDEX idx_sessions_user_status (user_id, status)

-- Captures : analyses temporelles
INDEX idx_captures_session_captured (session_id, captured_at)

-- Capture chatters : dÃ©doublonnage et lookups
INDEX idx_capture_chatters_capture_user (capture_id, twitch_user_id)

-- Jobs : polling worker optimisÃ©
INDEX idx_jobs_status_created (status, created_at)
```

### Connection Pooling

Configuration du pool de connexions MariaDB par service :

```yaml
# Gateway (nombreuses requÃªtes utilisateurs)
DB_MAX_OPEN_CONNS: 50
DB_MAX_IDLE_CONNS: 10

# Worker (charges ponctuelles)
DB_MAX_OPEN_CONNS: 20
DB_MAX_IDLE_CONNS: 5

# Analysis (requÃªtes lourdes)
DB_MAX_OPEN_CONNS: 30
DB_MAX_IDLE_CONNS: 10
```

### ParamÃ¨tres MariaDB

```yaml
command:
  - --max-connections=200          # Support de tous les services
  - --innodb-buffer-pool-size=512M # Cache des donnÃ©es chaudes
```

## ScalabilitÃ© Horizontale

### Services Scalables

Les services suivants **sont prÃªts pour le scaling** (pas de `container_name`) :

- âœ… **Gateway** : API HTTP, sessions dans Redis (stateless)
- âœ… **Worker** : Consomme jobs depuis MariaDB/Redis (queue distribuÃ©e)
- âœ… **Analysis** : Cache dans Redis (stateless)
- âœ… **Twitch-API** : Rate limiting dans Redis (partagÃ©)

### Services Non-Scalables

Ces services restent en **instance unique** :

- ğŸ”’ **MariaDB** : Base de donnÃ©es unique (voir section HA pour read replicas)
- ğŸ”’ **Redis** : Cache unique (suffisant pour la plupart des cas)
- ğŸ”’ **Traefik** : Reverse proxy unique

### Architecture Par DÃ©faut (1 Instance)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     Load Balancer (Traefik)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          v
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Gateway (x1)    â”‚  Stateless
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Worker (x1)     â”‚  Job Queue
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Analysis (x1)   â”‚  Cache
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Twitch-API (x1) â”‚  Rate Limiting
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Redis           â”‚  Cache/Sessions (partagÃ©)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ MariaDB         â”‚  Shared State (partagÃ©)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**CapacitÃ©** : 100-500 utilisateurs simultanÃ©s

## Scaling avec Docker Compose

### MÃ©thode 1 : Flag `--scale` (RecommandÃ©e pour Dev/Test)

```bash
# DÃ©marrer avec scaling
docker-compose up -d --scale gateway=2 --scale worker=3 --scale analysis=2

# VÃ©rifier les instances
docker-compose ps
# NAME                              STATUS
# twitch-chatters-analyser-gateway-1    running
# twitch-chatters-analyser-gateway-2    running
# twitch-chatters-analyser-worker-1     running
# twitch-chatters-analyser-worker-2     running
# twitch-chatters-analyser-worker-3     running
# twitch-chatters-analyser-analysis-1   running
# twitch-chatters-analyser-analysis-2   running

# ArrÃªter
docker-compose down
```

**Avantages** :
- âœ… Simple et rapide
- âœ… Pas de configuration supplÃ©mentaire
- âœ… IdÃ©al pour tests de charge

**InconvÃ©nients** :
- âŒ Il faut spÃ©cifier `--scale` Ã  chaque `up`
- âŒ Pas de scaling dynamique en cours d'exÃ©cution

### MÃ©thode 2 : Docker Swarm (RecommandÃ©e pour Production)

```bash
# 1. Initialiser Swarm
docker swarm init

# 2. DÃ©ployer la stack
docker stack deploy -c docker-compose.yml twitch-chatters

# 3. VÃ©rifier les services
docker service ls
# ID             NAME                        MODE         REPLICAS
# abc123         twitch-chatters_gateway     replicated   1/1
# def456         twitch-chatters_worker      replicated   1/1

# 4. Scaler dynamiquement (sans redÃ©marrage)
docker service scale twitch-chatters_gateway=3
docker service scale twitch-chatters_worker=5
docker service scale twitch-chatters_analysis=2

# 5. Surveiller
docker service ps twitch-chatters_gateway
# ID             NAME                          NODE      DESIRED STATE   CURRENT STATE
# xyz789         twitch-chatters_gateway.1     manager   Running         Running
# uvw012         twitch-chatters_gateway.2     manager   Running         Running
# rst345         twitch-chatters_gateway.3     manager   Running         Running

# 6. RÃ©duire le nombre de rÃ©plicas
docker service scale twitch-chatters_worker=2

# 7. Supprimer la stack
docker stack rm twitch-chatters
```

**Avantages** :
- âœ… Scaling dynamique sans redÃ©marrage
- âœ… Auto-restart des containers
- âœ… Health checks avancÃ©s
- âœ… Rolling updates
- âœ… Production-ready

**InconvÃ©nients** :
- âŒ NÃ©cessite Docker Swarm
- âŒ Syntaxe lÃ©gÃ¨rement diffÃ©rente

### Architecture Multi-RÃ©plicas

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     Load Balancer (Traefik)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
    â”Œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”
    â”‚     â”‚     â”‚
â”Œâ”€â”€â”€vâ”€â”€â”€â”€â”€vâ”€â”€â”€â”€â”€vâ”€â”€â”
â”‚ Gateway (x3)    â”‚  Stateless
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Worker (x5)     â”‚  Job Queue
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Analysis (x2)   â”‚  Cache
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Twitch-API (x1) â”‚  Rate Limiting (pas besoin de scale)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Redis           â”‚  Cache/Sessions (partagÃ©)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ MariaDB         â”‚  Shared State (partagÃ©)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**CapacitÃ©** : 500-1000 utilisateurs simultanÃ©s

## Redis - Cache DistribuÃ©

### Databases Redis (sÃ©paration logique)

```yaml
Gateway:    redis://redis:6379/0  # Sessions web
Twitch-API: redis://redis:6379/1  # Rate limiting distribuÃ©
Worker:     redis://redis:6379/2  # MÃ©tadonnÃ©es jobs
Analysis:   redis://redis:6379/3  # Cache des rÃ©sultats
```

### Utilisation du Cache

#### Dans Analysis (exemple)

```go
import "github.com/vignemail1/twitch-chatters-analyser/internal/redis"

// Initialisation
redisClient, err := redis.NewClient(os.Getenv("REDIS_URL"))
if err != nil {
    log.Fatal(err)
}
defer redisClient.Close()

// Cache des rÃ©sultats d'analyse
func (a *App) getSessionSummary(sessionUUID string) (*Summary, error) {
    cacheKey := "summary:" + sessionUUID
    
    // 1. VÃ©rifier le cache
    var summary Summary
    err := redisClient.GetJSON(ctx, cacheKey, &summary)
    if err == nil {
        return &summary, nil // Cache hit!
    }
    
    // 2. Calculer depuis la DB
    summary, err = a.computeSummaryFromDB(sessionUUID)
    if err != nil {
        return nil, err
    }
    
    // 3. Mettre en cache (5 minutes)
    ttl := 5 * time.Minute
    _ = redisClient.SetJSON(ctx, cacheKey, summary, ttl)
    
    return &summary, nil
}
```

#### Sessions Web (Gateway)

```go
// Stocker une session
sessionData := map[string]interface{}{
    "user_id": userID,
    "access_token": token,
}
redisClient.SetSession(ctx, sessionID, sessionData, 24*time.Hour)

// RÃ©cupÃ©rer une session
var session map[string]interface{}
err := redisClient.GetSession(ctx, sessionID, &session)
```

#### Rate Limiting DistribuÃ© (Twitch-API)

```go
// VÃ©rifier rate limit (10 req/sec)
allowed, err := redisClient.CheckRateLimit(ctx, "twitch-api", 10, 1*time.Second)
if !allowed {
    return errors.New("rate limit exceeded")
}
```

### Configuration Redis

```yaml
redis:
  command: redis-server
    --maxmemory 256mb              # Limite mÃ©moire
    --maxmemory-policy allkeys-lru # Ã‰viction LRU
```

## Configuration

### Variables d'Environnement

```bash
# Base de donnÃ©es
DB_MAX_OPEN_CONNS=50
DB_MAX_IDLE_CONNS=10

# Redis
REDIS_URL=redis://redis:6379/0

# Cache
CACHE_TTL_SECONDS=300  # 5 minutes

# Worker
JOB_POLL_INTERVAL=2    # 2 secondes

# Rate limiting
RATE_LIMIT_REQUESTS_PER_SECOND=10
```

### Ajustements selon la Charge

#### Charge faible (< 100 users)

```bash
# Configuration par dÃ©faut (1 replica par service)
docker-compose up -d
```

**CapacitÃ©** : 100-500 utilisateurs simultanÃ©s

#### Charge moyenne (100-1000 users)

```bash
# MÃ©thode 1: Compose --scale
docker-compose up -d --scale gateway=2 --scale worker=3 --scale analysis=2

# MÃ©thode 2: Swarm
docker swarm init
docker stack deploy -c docker-compose.yml twitch-chatters
docker service scale twitch-chatters_gateway=2
docker service scale twitch-chatters_worker=3
docker service scale twitch-chatters_analysis=2
```

**CapacitÃ©** : 500-1000 utilisateurs simultanÃ©s

#### Charge Ã©levÃ©e (> 1000 users)

```bash
# Swarm recommandÃ©
docker service scale twitch-chatters_gateway=4
docker service scale twitch-chatters_worker=5
docker service scale twitch-chatters_analysis=3
# + Envisager read replicas MariaDB
```

**CapacitÃ©** : > 1000 utilisateurs simultanÃ©s

## Monitoring

### MÃ©triques Ã  Surveiller

```bash
# Queue de jobs
docker exec twitch-chatters-db mariadb -u twitch -p -e \
  "SELECT status, COUNT(*) FROM jobs GROUP BY status;"

# Connexions DB actives
docker exec twitch-chatters-db mariadb -u twitch -p -e \
  "SHOW PROCESSLIST;"

# Utilisation Redis
docker exec twitch-chatters-redis redis-cli INFO memory

# Services actifs
docker-compose ps
# Ou en mode Swarm:
docker service ls
```

### Logs de Performance

```bash
# Logs avec timestamps (Compose)
docker-compose logs -f --tail=100 gateway
docker-compose logs -f --tail=100 worker

# Logs avec timestamps (Swarm)
docker service logs -f twitch-chatters_gateway
docker service logs -f twitch-chatters_worker

# Filtrer les requÃªtes lentes
docker-compose logs gateway | grep "in [0-9]\+ms" | awk '$NF > 1000'
```

### Signaux d'Alerte

âš ï¸ **Augmenter les workers** si :
- Queue de jobs > 100 pendant > 5 minutes
- Jobs `pending` > jobs `running` * 10

âš ï¸ **Augmenter les gateways** si :
- Latence HTTP > 500ms
- CPU gateway > 80%

âš ï¸ **Optimiser les requÃªtes DB** si :
- Connexions DB > 80% de max
- RequÃªtes > 100ms frÃ©quentes

## Gains de Performance Attendus

### Avec Optimisations Verticales

- **Indexes** : 2-5x plus rapide sur requÃªtes filtrÃ©es
- **Connection pool** : Ã‰limination des timeouts de connexion
- **Redis cache** : 100-1000x plus rapide (< 1ms vs 100-1000ms)

### Avec Replicas

- **Gateway x2** : 2x capacitÃ© HTTP (req/sec)
- **Worker x3** : 3x throughput jobs
- **Analysis x2** : 2x capacitÃ© analyses

### Avec Redis Cache

- **Cache hit** : 100-1000x plus rapide (< 1ms vs 100-1000ms)
- **RÃ©duction charge DB** : 50-80% selon taux de hit
- **Rate limiting distribuÃ©** : CohÃ©rence entre toutes les instances

## Ã‰volutions Futures

### Ã‰tape 1 : Auto-Scaling (Optionnel)

Pour scaling automatique basÃ© sur la charge :

1. **Kubernetes** : HorizontalPodAutoscaler
2. **Docker Swarm + Prometheus** : Scripts custom
3. **Cloud** : AWS ECS, GCP Cloud Run

### Ã‰tape 2 : Haute DisponibilitÃ©

1. **MariaDB Read Replicas**
   - SÃ©paration lecture/Ã©criture
   - Analysis et Gateway utilisent les replicas
   - 2-3x capacitÃ© lecture

2. **Galera Cluster**
   - 3 nÅ“uds MariaDB actif-actif
   - Haute disponibilitÃ©
   - Ã‰limination du SPOF

3. **Multi-Serveurs**
   - Docker Swarm ou Kubernetes
   - SÃ©paration physique des services
   - Isolation des ressources

4. **Analytics DÃ©diÃ©**
   - ClickHouse pour analytics massifs
   - Data warehouse sÃ©parÃ©
   - Exports pÃ©riodiques depuis MariaDB

## Exemples de Configuration

### Configuration 1 : Dev/Test (Par DÃ©faut)

```bash
docker-compose up -d
```

**Ressources** :
- CPU : 4 vCPU
- RAM : 4 GB
- CapacitÃ© : 100-500 users

### Configuration 2 : Production Moyenne

```bash
docker-compose up -d --scale gateway=2 --scale worker=3 --scale analysis=2
```

**Ressources** :
- CPU : 8 vCPU
- RAM : 12 GB
- CapacitÃ© : 500-1000 users

### Configuration 3 : Production Haute Charge

```bash
docker swarm init
docker stack deploy -c docker-compose.yml twitch-chatters
docker service scale twitch-chatters_gateway=4
docker service scale twitch-chatters_worker=5
docker service scale twitch-chatters_analysis=3
```

**Ressources** :
- CPU : 16 vCPU
- RAM : 24 GB
- CapacitÃ© : > 1000 users
